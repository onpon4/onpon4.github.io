<!DOCTYPE html>
<html lang="en">

<head>
    <title>Bits - The Diligent Circle</title>
    <meta charset="UTF-8">
    <meta name="keywords" content="onpon, onpon4">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="icon" type="image/png" href="../favicon.png">
</head>

<body>
    <h1 class="header">The Diligent Circle</h1>
    <nav class="header">
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="../about.html">About</a></li>
            <li><a href="../games.html">Games</a></li>
            <li class="current"><a href="../articles.html">Articles</a></li>
            <li><a href="../contact.html">Contact</a></li>
            <li><a href="https://liberapay.com/diligentcircle/">Liberapay</a></li>
            <li><a href="https://www.patreon.com/diligentcircle">Patreon</a></li>
        </ul>
    </nav>

    <main>
    <article>
        <h2>Bits</h2>
        <p>I just wanted to write a short article about this. To this day, there are still ignorant gen X'ers who honestly think that "bits" on a computer (usually a video game console, which is nothing more glamorous than a dedicated single-purpose computer) somehow refers to power or graphical quality. So I'm going to quickly crush this nonsense.</p>
        <p>So. Why did companies in the early 1990s boast about their consoles being 16-bit, 32-bit, 64-bit, etc? To get you to buy stuff. That's it. It has no basis in any sort of reality.</p>
        <p>So what is the benefit of having a larger word size (e.g. 16-bit instead of 8-bit)? It lets you process larger numbers more efficiently. The utility of this for any typical user plateaus at 64 bits because 64 bits is enough to easily work with numbers up to 9.22*10<sup>18</sup>, or over 9 <em>quintillion</em>. For reference, that many seconds is over 20 times the estimated age of the universe (13.8 billion years or around 400 quadrillion seconds).</p>
        <p>So no, none of your beloved consoles are 128-bit, 256-bit, or 512-bit. The only 128-bit processor are mainframes and there's no such thing as a 256-bit or 512-bit processor. These video game systems you're calling "128-bit" and so on are actually mostly 32-bit because even manipulating numbers larger than 32-bit processors can easily handle (above about 2.1 billion) didn't have much practical use before around 2010 or so. The Nintendo 64 did use a 64-bit processor (the only one prior to the PS4, Switch, and Xbox One), and it probably gained absolutely nothing from this considering the kind of work it was doing.</p>
        <p>In short, you fell for meaningless propaganda. Yes, it all really is that simple. Sorry to break the news to you.</p>
        <p>üïµÔ∏è</p>
    </article>
    </main>

    <div class="footer">
        <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="CC BY-SA" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png"></a><br>This page is licensed under the <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
    </div>
</body>

</html>
